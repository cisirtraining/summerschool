{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day1_Introduction_to_DL_framework3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9wpRokcQ6IglZX6d8mMtA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cisirtraining/summerschool/blob/master/Day1_Introduction_to_DL_framework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92TvDp8cmxhB",
        "colab_type": "text"
      },
      "source": [
        "**Object detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9au4HI6bqt1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#author abukhari\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from os import listdir\n",
        "#-----------------------\n",
        "\n",
        "color = (0, 255, 0) # BGR\n",
        "# (255,0,0) Blue\n",
        "# (0, 255, 0) Green\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('../model/haarcascade_frontalface_default.xml')\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    \n",
        "    #preprocess_input normalizes input in scale of [-1, +1]. You must apply same normalization in prediction.\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def loadVggFaceModel(): # class: Encapsulates the Mask myDeepFace model functionality.\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
        "\tmodel.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(ZeroPadding2D((1,1)))\n",
        "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "\tmodel.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Convolution2D(2622, (1, 1)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Activation('softmax'))\n",
        "\t\n",
        "\t#you can download pretrained weights from https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
        "\tfrom tensorflow.keras.models import model_from_json\n",
        "\tmodel.load_weights('../model/vgg_face_weights.h5')\n",
        "\t\n",
        "\tvgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n",
        "\t\n",
        "\treturn vgg_face_descriptor\n",
        "\n",
        "#put your employee pictures in this path as name_of_employee.jpg\n",
        "employee_pictures = \"./database\"\n",
        "\n",
        "employees = dict()\n",
        "\n",
        "for file in listdir(employee_pictures):\n",
        "\temployee, extension = file.split(\".\")\n",
        "\temployees[employee] = model.predict(preprocess_image('./database/%s.png' % (employee)))[0,:]\n",
        "\t\n",
        "print(\"employee representations retrieved successfully\")\n",
        "\n",
        "def findCosineSimilarity(source_representation, test_representation):\n",
        "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
        "    b = np.sum(np.multiply(source_representation, source_representation))\n",
        "    c = np.sum(np.multiply(test_representation, test_representation))\n",
        "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TjyBltLrC_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = loadVggFaceModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I93hyUalD7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap = cv2.VideoCapture(0) #webcam\n",
        "#cap = cv2.VideoCapture('C:/Users/IS96273/Desktop/zuckerberg.mp4') #video\n",
        "\n",
        "while(True):\n",
        "\tret, img = cap.read() # XXX: what 'img' format\n",
        "\t#img = cv2.resize(img, (640, 360))\n",
        "\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
        "\t\n",
        "\tfor (x,y,w,h) in faces:\n",
        "\t\tif w > 130: \n",
        "\t\t\t#cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) #draw rectangle to main image\n",
        "\t\t\t\n",
        "\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
        "\t\t\tdetected_face = cv2.resize(detected_face, (224, 224)) #resize to 224x224\n",
        "\t\t\t\n",
        "\t\t\timg_pixels = image.img_to_array(detected_face)\n",
        "\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\n",
        "\t\t\t#img_pixels /= 255\n",
        "\t\t\t#employee dictionary is using preprocess_image and it normalizes in scale of [-1, +1]\n",
        "\t\t\timg_pixels /= 127.5\n",
        "\t\t\timg_pixels -= 1\n",
        "\t\t\t\n",
        "\t\t\tcaptured_representation = model.predict(img_pixels)[0,:]\n",
        "\t\t\t\n",
        "\t\t\tfound = 0\n",
        "\t\t\tfor i in employees:\n",
        "\t\t\t\temployee_name = i\n",
        "\t\t\t\trepresentation = employees[i]\n",
        "\t\t\t\t\n",
        "\t\t\t\tsimilarity = findCosineSimilarity(representation, captured_representation)\n",
        "\t\t\t\tif(similarity < 0.30):\n",
        "\t\t\t\t\tcv2.putText(img, employee_name, (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tfound = 1\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\t\t\n",
        "\t\t\t#connect face and text\n",
        "\t\t\tcv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),color,1)\n",
        "\t\t\tcv2.line(img,(x+w,y-20),(x+w+10,y-20),color,1)\n",
        "\t\t\n",
        "\t\t\tif(found == 0): #if found image is not in employee database\n",
        "\t\t\t\tcv2.putText(img, 'unknown', (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
        "\t\n",
        "\tcv2.imshow('img',img)\n",
        "\t\n",
        "\tif cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
        "\t\tbreak\n",
        "\t\n",
        "#kill open cv things\t\t\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-Gfkg4ZEfjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "        // create html elements\n",
        "        const div = document.createElement('div');\n",
        "        const video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "\n",
        "        // request the stream. This will ask for Permission to access \n",
        "        // a connected Camera/Webcam\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "        // show the HTML elements\n",
        "        document.body.appendChild(div);\n",
        "        div.appendChild(video);\n",
        "        // display the stream\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        // Resize the output (of Colab Notebook Cell) to fit the video element.\n",
        "        google.colab.output\n",
        "            .setIfrmeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "        // capture 5 frames (for test)\n",
        "        for (let i = 0; i < 5; i++) {\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            img = canvas.toDataURL('image/jpeg', quality);\n",
        "\n",
        "            // Call a python function and send this image\n",
        "            google.colab.kernel.invokeFunction('notebook.run_algo', [img], {});\n",
        "\n",
        "            // wait for X miliseconds second, before next capture\n",
        "            await new Promise(resolve => setTimeout(resolve, 250));\n",
        "        }\n",
        "\n",
        "        stream.getVideoTracks()[0].stop(); // stop video stream\n",
        "    }\n",
        "        ''')\n",
        "    # make the provided HTML, part of the cell\n",
        "    display(js) \n",
        "    # call the takePhoto() JavaScript function\n",
        "    data = eval_js('takePhoto({})'.format(quality)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdHbCnukEiKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "import base64\n",
        "import logging\n",
        "\n",
        "from google.colab import output\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "def data_uri_to_img(uri):\n",
        "    \"\"\"convert base64 image to numpy array\"\"\"\n",
        "    try:\n",
        "        image = base64.b64decode(uri.split(',')[1], validate=True)\n",
        "        # make the binary image, a PIL image\n",
        "        image = Image.open(BytesIO(image))\n",
        "        # convert to numpy array\n",
        "        image = np.array(image, dtype=np.uint8); \n",
        "        return image\n",
        "    except Exception as e:\n",
        "        logging.exception(e);print('\\n')\n",
        "        return None\n",
        "\n",
        "def run_algo(imgB64):\n",
        "    \"\"\"\n",
        "    in Colab, run_algo function gets invoked by the JavaScript, \n",
        "    that sends N images every second, one at a time.\n",
        "\n",
        "    params:\n",
        "        image: image\n",
        "    \"\"\"\n",
        "    image = data_uri_to_img(imgB64)  \n",
        "    if image is None:\n",
        "        print(\"At run_algo(): image is None.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Run detection\n",
        "        results = model.detect([image], verbose=1)\n",
        "        # Visualize results\n",
        "        r = results[0]    \n",
        "        visualize.display_instances(\n",
        "            image, \n",
        "            r['rois'], \n",
        "            r['masks'], \n",
        "            r['class_ids'], \n",
        "            class_names, \n",
        "            r['scores']\n",
        "            )\n",
        "    except Exception as e:\n",
        "        logging.exception(e)\n",
        "        print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Cv2rr_Exuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# register this function, so JS code could call this\n",
        "output.register_callback('notebook.run_algo', run_algo)\n",
        "\n",
        "# put the JS code in cell and run it\n",
        "take_photo()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}