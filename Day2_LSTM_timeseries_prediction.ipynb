{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2_LSTM_timeseries_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cisirtraining/summerschool/blob/master/Day2_LSTM_timeseries_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04RZ2nRyDM2D",
        "colab_type": "text"
      },
      "source": [
        "##**Mount Google Drive for Data Access**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a68GisnQ9Org",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTFKNq4Gu8qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scikit-plot\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xTFlm5zvYEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "print('****** GPU Information: ******\\n')\n",
        "printm()\n",
        "\n",
        "#!kill -9 -1 #kill virtual machine and assign new one"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQgOo2ExNwik",
        "colab_type": "text"
      },
      "source": [
        "#**Detail Info on LSTM**\n",
        "Excellent blog post:\n",
        "[Reference](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "If you interested in maths behind LSTM, read the following (I warn you, think twice ^^):\n",
        "[Paper](https://arxiv.org/abs/1909.09586)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHSqtA4LmMod",
        "colab_type": "text"
      },
      "source": [
        "##Recurrent Neural Networks\n",
        "\n",
        "The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks that’s a very bad idea. If you want to predict the next word in a sentence you better know which words came before it. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations and you already know that they have a “memory” which captures information about what has been calculated so far.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1EW9HBneLrcleW1IBR0GE2VJrZhPG-pkc)\n",
        "## Issues of RNN:\n",
        "*   Vanishing gradient problem\n",
        "*   Exploding gradient problem\n",
        "\n",
        "## Long-Short Term Memory(LSTM)\n",
        "The LSTM has three gates that update and control the cell states, these are the forget gate, input gate and output gate. The gates use hyperbolic tangent and sigmoid activation functions. \n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1coyG3t6Mbe04WPn8xPdL-64zmdz1oidj)\n",
        "[Source](https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/)\n",
        "\n",
        "A block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block’s state and output. A block operates upon an input sequence and each gate within a block uses the sigmoid activation units to control whether they are triggered or not, making the change of state and addition of information flowing through the block conditional.\n",
        "\n",
        "There are three types of gates within a unit:\n",
        "\n",
        "    Forget Gate: conditionally decides what information to throw away from the block.\n",
        "    Input Gate: conditionally decides which values from the input to update the memory state.\n",
        "    Output Gate: conditionally decides what to output based on input and the memory of the block.\n",
        "\n",
        "\n",
        "The forget gate controls what information in the cell state to forget, given new information than entered the network.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1q-iNGMv_OGIuwpQ7xxtec_wwxo0rhB7j)\n",
        "[Source](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577)\n",
        "\n",
        "The input gate controls what new information will be encoded into the cell state, given the new input information.\n",
        "![alt text](https://drive.google.com/uc?id=17432J3U9VTGqsf76LzuH96XwkGjk-aji)\n",
        "[Source](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577)\n",
        "\n",
        "The output gate controls what information encoded in the cell state is sent to the network as input in the following time step, this is done via the output vector Ht.\n",
        "![alt text](https://drive.google.com/uc?id=1u7zWxzP__o0lEhuNzvUjufj886KEcAqf)\n",
        "[Source](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMyrtQqaC9Ev",
        "colab_type": "text"
      },
      "source": [
        "##**Dataset Description**\n",
        "\n",
        ">### Bonn Epileptic Dataset\n",
        "Source: http://epileptologie-bonn.de/cms/front_content.php?idcat=193&lang=3\n",
        " <figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1eIc9eI6ByIt7onVJ2pv6-EQ44qbbTYnT' />\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "* Bonn dataset is categorized into 5 subsets (A - E), each subset contains 100 single-channel EEG signals of 23.6 seconds obtained from 5 patients - recorded at 173.61 Hz with 12-bit resolution.   \n",
        "*   Example of raw signal is depicted in the figure with only set E contains epileptic seizure activity.\n",
        "&nbsp;&nbsp;\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1xhvMyPXe_-pkEOwl3Yf0qZkUyiFu6yp4' />\n",
        "</center>\n",
        "</figure>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRt99SdkDV6z",
        "colab_type": "text"
      },
      "source": [
        "##**Download EEG Dataset**\n",
        "\n",
        "*  dataset from Bonn University Epilepsy Dataset\n",
        "*   100 files for each set and 4097 samples per file\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWDD3Wwn9Yc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://epileptologie-bonn.de/cms/upload/workgroup/lehnertz/Z.zip  # Set A\n",
        "!wget http://epileptologie-bonn.de/cms/upload/workgroup/lehnertz/O.zip  # Set B\n",
        "!wget http://epileptologie-bonn.de/cms/upload/workgroup/lehnertz/N.zip  # Set C\n",
        "!wget http://epileptologie-bonn.de/cms/upload/workgroup/lehnertz/F.zip  # Set D\n",
        "!wget http://epileptologie-bonn.de/cms/upload/workgroup/lehnertz/S.zip  # Set E\n",
        "  \n",
        "!unzip Z.zip -d 'Set A'\n",
        "!unzip O.zip -d 'Set B'\n",
        "!unzip N.zip -d 'Set C'\n",
        "!unzip F.zip -d 'Set D'\n",
        "!unzip S.zip -d 'Set E'\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6atsLa0WNgPt",
        "colab_type": "text"
      },
      "source": [
        "##**Load Dataset**\n",
        "\n",
        "*   Load EEG data from Set A (Normal) and Set E (Epileptic) and their respective labels \n",
        "*   Label convention: Normal = 0 and Epileptic = 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE6d1LOq9Yhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATA_DIR_A = './Set A/'\n",
        "DATA_DIR_B = './Set B/'\n",
        "DATA_DIR_C = './Set C/'\n",
        "DATA_DIR_D = './Set D/'\n",
        "DATA_DIR_E = './Set E/'\n",
        "\n",
        "LABEL_C1 = 0\n",
        "LABEL_C2 = 1\n",
        "\n",
        "def load_data():\n",
        "    data = []\n",
        "    nbFiles = 0\n",
        "    for fname in tqdm(os.listdir(DATA_DIR_A)):\n",
        "        img = np.loadtxt(DATA_DIR_A + fname)\n",
        "        data.append([np.array(img), np.array(LABEL_C1)])\n",
        "        nbFiles+=1\n",
        "        \n",
        "    for fname in tqdm(os.listdir(DATA_DIR_E)):\n",
        "        img = np.loadtxt(DATA_DIR_E + fname)\n",
        "        data.append([np.array(img), np.array(LABEL_C2)])\n",
        "        nbFiles+=1\n",
        "        \n",
        "    return data\n",
        "\n",
        "\n",
        "data = load_data()\n",
        "print('\\n')\n",
        "print('Total Files: {}'.format(len(data)))\n",
        "print('Shape of data: {}'.format(np.shape(data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2apUIEIHOl3a",
        "colab_type": "text"
      },
      "source": [
        "##**Sanity Check - Visualize Subset from Set A and E**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koxNE8R49Ykw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data visualization\n",
        "\n",
        "import random\n",
        "mpl.rcParams['figure.figsize'] = (20,10)\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "samples_c1 = np.empty((0, 4097), int)\n",
        "while len(samples_c1) < 4:\n",
        "    curID = random.randint(0, len(data)) # random id between 0-200\n",
        "    if data[curID][1] == LABEL_C1:\n",
        "        samples_c1 = np.vstack((samples_c1, data[curID][0]))\n",
        "        \n",
        "samples_c2 = np.empty((0, 4097), int)\n",
        "while len(samples_c2) < 4:\n",
        "    curID = random.randint(0, len(data))\n",
        "    if data[curID][1] == LABEL_C2:\n",
        "        samples_c2 = np.vstack((samples_c2, data[curID][0]))\n",
        "        \n",
        "for i in range(0, 4):\n",
        "    plt.subplot(421 + i * 2)\n",
        "    plt.plot(samples_c1[i])\n",
        "    plt.subplot(421 + i * 2 + 1)\n",
        "    plt.plot(samples_c2[i])\n",
        "\n",
        "ax = plt.subplot(421)\n",
        "ax.set_title(\"CLASS 1\")\n",
        "ax = plt.subplot(422)\n",
        "ax.set_title(\"CLASS 2\")\n",
        "\n",
        "fig.suptitle(\"Set A vs Set E\", fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxnHGhKHO1ep",
        "colab_type": "text"
      },
      "source": [
        "##**Data Preparation**\n",
        "\n",
        "*   Split data into training and testing set, 80% training and 20% testing \n",
        "*   Convert the label to one-hot-encoded vectors: 0 >> [1,0] and 1 >> [0,1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0UNxqDs9YnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data preparation\n",
        "from sklearn.utils import shuffle\n",
        "data = shuffle(data)\n",
        "nb_train = round(len(data) * 0.8)\n",
        "data_train = data[0:nb_train]\n",
        "data_test = data[nb_train:]\n",
        "\n",
        "print(data_train[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L_ZclR3ZKBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.array([d[0] for d in data_train])\n",
        "Y_train = np.array([d[1] for d in data_train])\n",
        "\n",
        "X_test = np.array([d[0] for d in data_test])\n",
        "Y_test = np.array([d[1] for d in data_test])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)\n",
        "print('X_test',X_test)\n",
        "print('\\nY_test',Y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM8ECJDOSNPQ",
        "colab_type": "text"
      },
      "source": [
        "The LSTM network expects the input data (X) to be provided with a specific array structure in the form of:\n",
        "`[samples, time steps, features]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laO48cJZZNqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 4097, 1)\n",
        "Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], 4097, 1)\n",
        "Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
        "\n",
        "\n",
        "print('Training data :',X_train.shape)\n",
        "print('Training label:',Y_train.shape)\n",
        "print('Testing data  :',X_test.shape)\n",
        "print('Testing label :',Y_test.shape)\n",
        "\n",
        "print('\\nOriginal training label:\\n',Y_train[0:3])\n",
        "\n",
        "\n",
        "# Encode labels to hot vectors (ex : 2 -> 0=[1,0] and 1=[0,1])\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "Y_train = to_categorical(Y_train, num_classes = 2)\n",
        "Y_test = to_categorical(Y_test, num_classes = 2)\n",
        "\n",
        "\n",
        "print('\\nOne-hot encoded label:\\n',Y_train[0:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKqSVXibPiNJ",
        "colab_type": "text"
      },
      "source": [
        "##**Model Training**\n",
        ">>### Long Short Term Memory (LSTM)\n",
        "\n",
        ">>![](https://drive.google.com/uc?id=1coyG3t6Mbe04WPn8xPdL-64zmdz1oidj)\n",
        "\n",
        "[Source](https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFZXf3L9Yqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation,BatchNormalization, LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "\n",
        "batch_size =16\n",
        "nb_epoch = 25\n",
        "hidden_size = 128\n",
        "use_dropout=True\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=30, verbose=0, mode='min')\n",
        "chckpoint = ModelCheckpoint('./drive/My Drive/data/cuDNNlstm_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_size, input_shape=(4097,1)))\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['mae', 'acc'])\n",
        "print(model.summary())\n",
        "\n",
        "history = model.fit(X_train, Y_train, verbose=1, validation_data=(X_test, Y_test),shuffle=True, batch_size=batch_size, epochs=nb_epoch, callbacks=[earlyStopping,chckpoint])\n",
        "\n",
        "model.save('./drive/My Drive/data/cuDNNlstm_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_t1tysSqK_H",
        "colab_type": "text"
      },
      "source": [
        "##Plot Learning Curves\n",
        "Should have something like this!\n",
        "![alt text](https://drive.google.com/uc?id=1VysuZaAPPOYOvkP-BJ0Wpj-9oKNt8oyF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkYR2cQwH-3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(1, figsize = (15,8)) \n",
        "    \n",
        "plt.subplot(221)  \n",
        "plt.plot(history.history['acc'])  \n",
        "plt.plot(history.history['val_acc'])  \n",
        "plt.title('model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'valid'],loc='lower right') \n",
        "    \n",
        "plt.subplot(222)  \n",
        "plt.plot(history.history['loss'])  \n",
        "plt.plot(history.history['val_loss'])  \n",
        "plt.title('model loss')  \n",
        "plt.ylabel('loss')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'valid'], loc='upper right') \n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9CtnGd2GsZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at confusion matrix \n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize = (5,5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MpZz-4kT7T9",
        "colab_type": "text"
      },
      "source": [
        "##**Standard Evaluation Metrics**\n",
        "\n",
        "\n",
        "\n",
        "$$Precision = \\frac{TP}{TP+FP} $$\n",
        "\n",
        "$$Recall = \\frac{TP}{TP+FN} $$\n",
        "\n",
        "$$F1 = 2 * \\frac{Precision*Recall}{Precision + Recall}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfmaXrUlSMNZ",
        "colab_type": "text"
      },
      "source": [
        "##**Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_86uA7MtGswx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import scikitplot\n",
        "from keras.models import load_model, Model\n",
        "\n",
        "# Load trained network\n",
        "model = load_model('./drive/My Drive/data/cuDNNlstm_model.h5')    # ./drive/My Drive/data/cuDNNlstm_model.h5\n",
        "\n",
        "print('\\n****************** model loaded *******************')\n",
        "\n",
        "batch_size = 8\n",
        "# Evaluate the trained model\n",
        "score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
        "print(\"\\n[info] loss={:.3f}, accuracy: {:.3f}%\\n\".format(score[0],score[2] * 100))\n",
        "\n",
        "# Make prediction\n",
        "y_pred = model.predict(X_test) \n",
        "map_characters = {0: 'normal', 1: 'epileptic'}\n",
        "print('\\n', sklearn.metrics.classification_report(np.where(Y_test > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')\n",
        "\n",
        "Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
        "Y_true = np.argmax(Y_test,axis = 1) \n",
        "\n",
        "# plot confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))\n",
        "\n",
        "\n",
        "# Plot roc curve:\n",
        "scikitplot.metrics.plot_roc(np.argmax(Y_test, axis=1), y_pred, figsize=(10,8))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxUaCUnDSb5d",
        "colab_type": "text"
      },
      "source": [
        "##**Visual Sanity Check**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_08dDl7W7gC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import random\n",
        "\n",
        "idx = random.sample(range(0,40), 6) # regenerate random index\n",
        "\n",
        "for i in range(6):\n",
        "    (normal, epileptic) = model.predict(X_test)[idx[i]]\n",
        "    label = \"normal\" if normal > epileptic else \"epileptic\"\n",
        "    proba = epileptic if epileptic > normal else normal\n",
        "    label = \"{}: {:.2f}%\".format(label, proba * 100)\n",
        "    true_label = 'normal' if (Y_test[idx[i]] == 1)[0] else 'epileptic'\n",
        "   \n",
        "    plt.subplot(611+i)\n",
        "    plt.plot(X_test[idx[i]])\n",
        "    plt.title('True:{} (Predicted:{})'.format(true_label,label))\n",
        "    plt.axis('on')\n",
        "    plt.tight_layout()\n",
        "         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e82-7favu_AE",
        "colab_type": "text"
      },
      "source": [
        "#**LSTM for Regression**\n",
        "\n",
        "Credit: [Reference](https://machinelearningmastery.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tt6w56awipy",
        "colab_type": "text"
      },
      "source": [
        "##**Dataset Description**\n",
        "\n",
        "###International Airline Passengers time-series prediction problem\n",
        "Problem: Given a year and a month, the task is to predict the number of international airline passengers in units of 1,000. The data ranges from January 1949 to December 1960, or 12 years, with 144 observations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BXmqJrOyQuQ",
        "colab_type": "text"
      },
      "source": [
        "###**Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzXAODVURE9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "airline = pd.read_csv('/content/drive/My Drive/data/airline-passengers.csv', parse_dates= True, index_col='Month')\n",
        "airline.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka7GDbDxTC4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(airline)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Passengers')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzSyWRsZRE4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# load the dataset\n",
        "airline = pd.read_csv('/content/drive/My Drive/data/airline-passengers.csv', usecols=[1])\n",
        "airline = airline.values\n",
        "airline = airline.astype('float32')\n",
        "\n",
        "airline[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKy0Lxck207K",
        "colab_type": "text"
      },
      "source": [
        "LSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It can be a good practice to rescale the data to the range of 0-to-1, also called normalizing. We can easily normalize the dataset using the MinMaxScaler preprocessing class from the scikit-learn library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpc7qnH3RE1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "airline = scaler.fit_transform(airline)\n",
        "\n",
        "print('normalized:',airline[:10])\n",
        "print('max value:',airline.max())\n",
        "print('min value',airline.min())\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(airline)\n",
        "plt.ylabel('Passengers (Normalized)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psck_lE1REzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split into train and test sets\n",
        "train_size = int(len(airline) * 0.7)\n",
        "test_size = len(airline) - train_size\n",
        "train, test = airline[0:train_size,:], airline[train_size:len(airline),:]\n",
        "\n",
        "print('training sample:',len(train))\n",
        "print('test sample:',len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p93abKyI8I-a",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   With time series data, the sequence of values are important. \n",
        "2.   We will define a function to create new dataset which takes two arguments: the` dataset`, which is a NumPy array that we want to convert into a dataset, and the `look_back`, which is the number of previous time steps to use as input variables to predict the next time period — in this case defaulted to 1.\n",
        "\n",
        "3. This default will create a dataset where `X` is the number of passengers at a given time (t) and `Y` is the number of passengers at the next time (t + 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT-8sOTiREwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back-1):\n",
        "\t\ta = dataset[i:(i+look_back), 0]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back, 0])\n",
        "\treturn np.array(dataX), np.array(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRdNqbxn8qzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "look_back = 1\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "\n",
        "print('trainX shape:',trainX.shape)\n",
        "print('trainY shape:',trainY.shape)\n",
        "print('testX shape:',testX.shape)\n",
        "print('testY shape:',testY.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkr8x6fN-ypt",
        "colab_type": "text"
      },
      "source": [
        "The LSTM network expects the input data (X) to be provided with a specific array structure in the form of:\n",
        "`[samples, time steps, features]`.\n",
        "\n",
        "Currently, our data is in the form: `[samples, features]` and we are framing the problem as one time step for each sample. We can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xkPSqSYREt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "\n",
        "print('trainX shape:',trainX.shape)\n",
        "print('testX shape:',testX.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx3Yp44FRErF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(16, input_shape=(1, look_back)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "print(model.summary())\n",
        "\n",
        "history = model.fit(trainX, trainY, epochs=150, batch_size=1, verbose=2)\n",
        "\n",
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMu0NEEGREoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(1, figsize = (5,5)) \n",
        "plt.plot(history.history['loss'])  \n",
        "plt.title('model loss')  \n",
        "plt.ylabel('loss')  \n",
        "plt.xlabel('epoch')  \n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLxkdI6dBwGT",
        "colab_type": "text"
      },
      "source": [
        "Note that we invert the predictions before calculating error scores to ensure that performance is reported in the same units as the original data (thousands of passengers per month)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZnzCHg8Bo6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make predictions\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "\n",
        "# invert predictions (invert back to normal data form without normalization)\n",
        "trainPredict = scaler.inverse_transform(trainPredict)\n",
        "trainY = scaler.inverse_transform([trainY])\n",
        "testPredict = scaler.inverse_transform(testPredict)\n",
        "testY = scaler.inverse_transform([testY])\n",
        "\n",
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "\n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaZd1e0lD8vV",
        "colab_type": "text"
      },
      "source": [
        "Because of how the dataset was prepared, we must shift the predictions so that they align on the x-axis with the original dataset. Once prepared, the data is plotted, showing the original dataset in blue, the predictions for the training dataset in orange, and the predictions on the unseen test dataset in green."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GDNhsR3BoxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(airline)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(airline)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(airline)-1, :] = testPredict\n",
        "\n",
        "# plot baseline and predictions\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(scaler.inverse_transform(airline))\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Passengers (in thousand)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48czTriwMtt_",
        "colab_type": "text"
      },
      "source": [
        "###**Use specific window size for prediction:**\n",
        "\n",
        "For example, given the current time (t) we want to predict the value at the next time in the sequence (t+1), we can use the current time (t), as well as the two prior times (t-1) and (t-2) as input variables.\n",
        "\n",
        "When phrased as a regression problem, the input variables are t-2, t-1, t and the output variable is t+1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAedlV99NY2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = {'X(t)':[112,118,132,129,121], 'X(t-1)':[118,132,129,121,135], 'X(t-2)':[132,129,121,135,148], 'X(t+1)':[129,121,135,148,148]}\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}